{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "MNIST_FTT.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gibsonx/CE888/blob/master/MNIST_FTT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs9U8-Bedlyu"
      },
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import pandas as pd\n",
        "from tensorflow_federated.python.simulation import FileCheckpointManager\n",
        "from tensorflow import reshape, nest, config\n",
        "from tensorflow.keras import losses, metrics, optimizers\n",
        "# Test the TFF is working:\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import os\n",
        "import secrets\n",
        "import logging\n",
        "import boto3\n",
        "from botocore.client import Config,ClientError\n",
        "import tarfile\n",
        "tff.federated_computation(lambda: 'Hello, World!')()\n",
        "print(tf.__version__)\n",
        "\n",
        "train_seq = 1\n",
        "bucket_name = secrets.token_hex(nbytes=8)\n",
        "experiment_name = \"mnist\"\n",
        "method = \"tff_training\"\n",
        "client_lr = 1e-2\n",
        "server_lr = 1e-2\n",
        "split = 4\n",
        "NUM_ROUNDS = 5\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 1\n",
        "PREFETCH_BUFFER = 10\n",
        "\n",
        "df_orig_train = pd.read_csv('mnist_train.csv')\n",
        "df_orig_test = pd.read_csv('mnist_test.csv')\n",
        "\n",
        "x_train = df_orig_train.iloc[:,1:].to_numpy().astype(np.float32).reshape(59999,28,28,1)[:10]\n",
        "y_train = df_orig_train.iloc[:,0].to_numpy().astype(np.int32).reshape(59999,1)[:10]\n",
        "x_test = df_orig_test.iloc[:,1:].to_numpy().astype(np.float32).reshape(9999,28,28,1)[:10]\n",
        "y_test = df_orig_test.iloc[:,0].to_numpy().astype(np.int32).reshape(9999,1)[:10]\n",
        "\n",
        "total_image_count = len(x_train)\n",
        "image_per_set = int(np.floor(total_image_count/split))\n",
        "\n",
        "client_train_dataset = collections.OrderedDict()\n",
        "for i in range(1, split+1):\n",
        "    client_name = \"client_\" + str(i)\n",
        "    start = image_per_set * (i-1)\n",
        "    end = image_per_set * i\n",
        "\n",
        "    print(f\"Adding data from {start} to {end} for client : {client_name}\")\n",
        "    data = collections.OrderedDict((('label', y_train[start:end]), ('pixels', x_train[start:end])))\n",
        "    client_train_dataset[client_name] = data\n",
        "\n",
        "train_dataset = tff.simulation.datasets.TestClientData(client_train_dataset)\n",
        "\n",
        "sample_dataset = train_dataset.create_tf_dataset_for_client(train_dataset.client_ids[0])\n",
        "sample_element = next(iter(sample_dataset))\n",
        "\n",
        "SHUFFLE_BUFFER = image_per_set\n",
        "\n",
        "def preprocess(dataset):\n",
        "\n",
        "  def batch_format_fn(element):\n",
        "    \"\"\"Flatten a batch `pixels` and return the features as an `OrderedDict`.\"\"\"\n",
        "    print(element['pixels'])\n",
        "    return collections.OrderedDict(\n",
        "        x=reshape(element['pixels'], [-1, 28, 28, 1]),\n",
        "        y=reshape(element['label'], [-1, 1]))\n",
        "\n",
        "  return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER).batch(\n",
        "      BATCH_SIZE).map(batch_format_fn).prefetch(PREFETCH_BUFFER)\n",
        "\n",
        "preprocessed_sample_dataset = preprocess(sample_dataset)\n",
        "sample_batch = nest.map_structure(lambda x: x.numpy(), next(iter(preprocessed_sample_dataset)))\n",
        "\n",
        "def make_federated_data(client_data, client_ids):\n",
        "    return [preprocess(client_data.create_tf_dataset_for_client(x)) for x in client_ids]\n",
        "\n",
        "federated_train_data = make_federated_data(train_dataset, train_dataset.client_ids)\n",
        "print('Number of client datasets: {l}'.format(l=len(federated_train_data)))\n",
        "print('First dataset: {d}'.format(d=federated_train_data[0]))\n",
        "\n",
        "def create_keras_model():\n",
        "    model = Sequential([\n",
        "        Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same',activation ='relu', input_shape = (28,28,1)),\n",
        "        Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same',activation ='relu'),\n",
        "        MaxPool2D(pool_size=(2,2)),\n",
        "        Dropout(0.25),\n",
        "        Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same',activation ='relu'),\n",
        "        Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same',activation ='relu'),\n",
        "        MaxPool2D(pool_size=(2,2), strides=(2,2)),\n",
        "        Dropout(0.3),\n",
        "        Flatten(),\n",
        "        Dense(512, activation = \"relu\", use_bias= True),\n",
        "        Dropout(0.5),\n",
        "        Dense(10, activation = \"softmax\")\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def model_fn():\n",
        "  keras_model = create_keras_model()\n",
        "  return tff.learning.from_keras_model(\n",
        "      keras_model,\n",
        "      input_spec=preprocessed_sample_dataset.element_spec,\n",
        "      loss=losses.SparseCategoricalCrossentropy(),\n",
        "      metrics=[metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "iterative_process = tff.learning.build_federated_averaging_process(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: optimizers.Adam(learning_rate=client_lr),\n",
        "    server_optimizer_fn=lambda: optimizers.SGD(learning_rate=server_lr))\n",
        "\n",
        "print(str(iterative_process.initialize.type_signature))\n",
        "\n",
        "eval_model = None\n",
        "\n",
        "state = iterative_process.initialize()\n",
        "\n",
        "eval_model = None\n",
        "for round_num in range(1, NUM_ROUNDS+1):\n",
        "    state, tff_metrics = iterative_process.next(state, federated_train_data)\n",
        "    print('round {:2d}, metrics={}'.format(round_num, tff_metrics))\n",
        "\n",
        "\n",
        "filePath = './' + bucket_name + '/'\n",
        "\n",
        "ckpt_manager = FileCheckpointManager(filePath)\n",
        "ckpt_manager.save_checkpoint(state,round_num=1)\n",
        "\n",
        "objFolder = 'ckpt_'+ str(train_seq)\n",
        "obj = 'ckpt_'+ str(train_seq) + '.tar.gz'\n",
        "\n",
        "tarPath =  filePath + objFolder\n",
        "tarFile = filePath  + obj\n",
        "# os.mkdir(filePath)\n",
        "# os.mkdir(filePath + '/ckpt_' + str(train_seq))\n",
        "\n",
        "def tardir(path, tar_name):\n",
        "    with tarfile.open(tar_name, \"w:gz\") as tar_handle:\n",
        "        for root, dirs, files in os.walk(path):\n",
        "            for file in files:\n",
        "                tar_handle.add(os.path.join(root, file))\n",
        "\n",
        "tardir(tarPath, tarFile)\n",
        "\n",
        "s3_client = boto3.resource('s3',\n",
        "                           endpoint_url='http://localhost:9000',\n",
        "                           aws_access_key_id='minio',\n",
        "                           aws_secret_access_key='minio123'\n",
        "                           )\n",
        "\n",
        "def create_bucket(bucket_name):\n",
        "    response = s3_client.buckets.all()\n",
        "    is_exist = False\n",
        "    for bucket in response:\n",
        "        if bucket.name == bucket_name:\n",
        "            is_exist == True\n",
        "    if is_exist == False:\n",
        "        try:\n",
        "           s3_client.create_bucket(Bucket=bucket_name)\n",
        "        except ClientError as e:\n",
        "            logging.error(e)\n",
        "            return False\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def upload_file(file_name, bucket, object_name=None):\n",
        "    if object_name is None:\n",
        "        object_name = file_name\n",
        "    try:\n",
        "        # response = s3_client.upload_fileobj(file_name, bucket, object_name)\n",
        "        response = s3_client.Object(bucket, file_name).upload_file(object_name)\n",
        "    except ClientError as e:\n",
        "        logging.error(e)\n",
        "        return False\n",
        "    return True\n",
        "create_bucket(bucket_name)\n",
        "\n",
        "upload_file(obj, bucket=bucket_name, object_name=tarFile)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
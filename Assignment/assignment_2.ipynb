{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_analysis_twitter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gibsonx/CE888/blob/master/Assignment/assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk0hIUcdXpi2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69fd718f-377d-4bd6-cee6-42b68db1f447"
      },
      "source": [
        "!pip install -U tensorflow tensorflow_datasets tensorflow_text zhon bert-for-tf2 sentencepiece transformers\n",
        "# !wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n",
        "!git clone https://github.com/cardiffnlp/tweeteval.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorflow in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Requirement already up-to-date: tensorflow_datasets in /usr/local/lib/python3.7/dist-packages (4.2.0)\n",
            "Requirement already up-to-date: tensorflow_text in /usr/local/lib/python3.7/dist-packages (2.4.3)\n",
            "Requirement already up-to-date: zhon in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already up-to-date: bert-for-tf2 in /usr/local/lib/python3.7/dist-packages (0.14.9)\n",
            "Requirement already up-to-date: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n",
            "Requirement already up-to-date: transformers in /usr/local/lib/python3.7/dist-packages (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied, skipping upgrade: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied, skipping upgrade: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (20.3.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.29.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (5.1.2)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (0.11.0)\n",
            "Requirement already satisfied, skipping upgrade: py-params>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from bert-for-tf2) (0.10.2)\n",
            "Requirement already satisfied, skipping upgrade: params-flow>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.3)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.28.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (54.2.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.53.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow_datasets) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7.2)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "fatal: destination path 'tweeteval' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WFgb6kZXX8C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bc1bf63-ee42-4918-b176-3a992be11740"
      },
      "source": [
        "import os,pathlib,re,nltk,pickle,string\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as text\n",
        "from tensorflow.keras import Input, layers, losses, preprocessing, utils\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.layers.core import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix\n",
        "from zhon import hanzi \n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import csv\n",
        "from tqdm.notebook import tqdm\n",
        "# import tokenization\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# import tensorflow_hub as hub\n",
        "# import tensorflow_datasets as tfds\n",
        "# from collections import Counter\n",
        "# from nltk.stem import WordNetLemmatizer\n",
        "# from google.colab import output\n",
        "# from google.colab import drive\n",
        "\n",
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEXTeQYlY9Ss"
      },
      "source": [
        "# drive.mount('/content/drive/')\n",
        "base_dir = '/content/tweeteval/datasets/'\n",
        "hate_dir = base_dir + \"hate\"\n",
        "irony_dir = base_dir + \"irony\"\n",
        "offensive_dir = base_dir + \"offensive\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcveRCcrB1Kh"
      },
      "source": [
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    \n",
        "    return emoji_pattern.sub(r'', string)\n",
        "\n",
        "def preprocess_tweet_text(tweet):\n",
        "    tweet = tweet.lower()\n",
        "    # Remove urls\n",
        "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n",
        "    # Remove user @ references and '#' from tweet\n",
        "    tweet = re.sub(r'\\@\\w+|\\#','', tweet)\n",
        "    # Remove punctuations\n",
        "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
        "    # tweet = tweet.translate(str.maketrans('', '', hanzi.punctuation))\n",
        "    tweet = remove_emoji(tweet)\n",
        "    tweet_tokens = word_tokenize(tweet)\n",
        "    # Remove stopwords\n",
        "    filtered_words = [w for w in tweet_tokens]\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "def readfile(text):\n",
        "  pd_list = []\n",
        "  with open(text,'r') as f:\n",
        "    for tweet in f.read().splitlines():\n",
        "      tweet = preprocess_tweet_text(tweet)\n",
        "      pd_list.append(tweet)   \n",
        "  return pd_list\n",
        " \n",
        "def readfile_label(text):\n",
        "  pd_list = []\n",
        "  with open(text,'r') as f:\n",
        "    for tweet in f.read().splitlines():\n",
        "      pd_list.append(int(tweet))  \n",
        "  return pd_list"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q0IWHEhEScP"
      },
      "source": [
        "#Pre-process csv and merge into a Dataframe\n",
        "#Hate DataFrame Data \n",
        "hate_dict_train = {'text': readfile(os.path.join(hate_dir,\"train_text.txt\")),'label': readfile_label(os.path.join(hate_dir,\"train_labels.txt\"))}\n",
        "hate_dict_val = {'text': readfile(os.path.join(hate_dir,\"val_text.txt\")),'label': readfile_label(os.path.join(hate_dir,\"val_labels.txt\"))}\n",
        "hate_dict_test = {'text': readfile(os.path.join(hate_dir,\"test_text.txt\")),'label': readfile_label(os.path.join(hate_dir,\"test_labels.txt\"))}\n",
        "hate_train_df = pd.DataFrame(hate_dict_train)\n",
        "hate_val_df = pd.DataFrame(hate_dict_val)\n",
        "hate_test_df = pd.DataFrame(hate_dict_test)\n",
        "df_hate_train = pd.concat([hate_train_df, hate_val_df]).reset_index(drop=True)\n",
        "not_hate = df_hate_train[df_hate_train['label'] == 0]\n",
        "hate = df_hate_train[df_hate_train['label'] == 1]\n",
        "\n",
        "#Irony DataFrame\n",
        "irony_dict_train = {'text': readfile(os.path.join(irony_dir,\"train_text.txt\")),'label': readfile_label(os.path.join(irony_dir,\"train_labels.txt\"))}\n",
        "irony_dict_val = {'text': readfile(os.path.join(irony_dir,\"val_text.txt\")),'label': readfile_label(os.path.join(irony_dir,\"val_labels.txt\"))}\n",
        "irony_dict_test = {'text': readfile(os.path.join(irony_dir,\"test_text.txt\")),'label': readfile_label(os.path.join(irony_dir,\"test_labels.txt\"))}\n",
        "irony_train_df = pd.DataFrame(irony_dict_train)\n",
        "irony_val_df = pd.DataFrame(irony_dict_val)\n",
        "irony_test_df = pd.DataFrame(irony_dict_test)\n",
        "irony_frames = [irony_train_df,irony_val_df,irony_test_df]\n",
        "df_irony = pd.concat(irony_frames)\n",
        "not_irony  = df_irony[df_irony['label'] == 0]\n",
        "irony  = df_irony[df_irony['label'] == 1]\n",
        "\n",
        "#Offensive DataFrame\n",
        "offensive_dict_train = {'text': readfile(os.path.join(offensive_dir,\"train_text.txt\")),'label': readfile_label(os.path.join(offensive_dir,\"train_labels.txt\"))}\n",
        "offensive_dict_val = {'text': readfile(os.path.join(offensive_dir,\"val_text.txt\")),'label': readfile_label(os.path.join(offensive_dir,\"val_labels.txt\"))}\n",
        "offensive_dict_test = {'text': readfile(os.path.join(offensive_dir,\"test_text.txt\")),'label': readfile_label(os.path.join(offensive_dir,\"test_labels.txt\"))}\n",
        "offensive_train_df = pd.DataFrame(offensive_dict_train)\n",
        "offensive_val_df = pd.DataFrame(offensive_dict_val)\n",
        "offensive_test_df = pd.DataFrame(offensive_dict_test)\n",
        "offensive_frames = [offensive_train_df,offensive_val_df,offensive_test_df]\n",
        "df_offensive = pd.concat(offensive_frames)\n",
        "not_offensive  = df_offensive[df_offensive['label'] == 0]\n",
        "offensive  = df_offensive[df_offensive['label'] == 1]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_18embbHU9R"
      },
      "source": [
        "from transformers import TFXLNetModel, XLNetTokenizer\n",
        "xlnet_model = 'xlnet-large-cased'\n",
        "xlnet_tokenizer = XLNetTokenizer.from_pretrained(xlnet_model)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqnGi96JHa6S"
      },
      "source": [
        "def create_xlnet(mname):\n",
        "    \"\"\" Creates the model. It is composed of the XLNet main block and then\n",
        "    a classification head its added\n",
        "    \"\"\"\n",
        "    # Define token ids as inputs\n",
        "    word_inputs = tf.keras.Input(shape=(120,), name='word_inputs', dtype='int32')\n",
        "\n",
        "    # Call XLNet model\n",
        "    xlnet = TFXLNetModel.from_pretrained(mname)\n",
        "    xlnet_encodings = xlnet(word_inputs)[0]\n",
        "\n",
        "    # CLASSIFICATION HEAD \n",
        "    # Collect last step from last hidden state (CLS)\n",
        "    doc_encoding = tf.squeeze(xlnet_encodings[:, -1:, :], axis=1)\n",
        "    # Apply dropout for regularization\n",
        "    doc_encoding = tf.keras.layers.Dropout(.1)(doc_encoding)\n",
        "    # Final output \n",
        "    outputs = tf.keras.layers.Dense(1, activation='sigmoid', name='outputs')(doc_encoding)\n",
        "\n",
        "    # Compile model\n",
        "    model = tf.keras.Model(inputs=[word_inputs], outputs=[outputs])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhKOF0i_HdE-",
        "outputId": "08f58cd1-2172-452c-f463-027d39bd9d33"
      },
      "source": [
        "xlnet = create_xlnet(xlnet_model)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at xlnet-large-cased were not used when initializing TFXLNetModel: ['lm_loss']\n",
            "- This IS expected if you are initializing TFXLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFXLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFXLNetModel were initialized from the model checkpoint at xlnet-large-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLNetModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f48200fdde0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f48200fdde0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f483b9a9c20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7f483b9a9c20> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LUW3oWeOYaA"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df_hate_train['text'], df_hate_train['label'], test_size=0.15, random_state=60)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPK_vhDmPP8a"
      },
      "source": [
        "def get_inputs(tweets, tokenizer, max_len=120):\n",
        "    \"\"\" Gets tensors from text using the tokenizer provided\"\"\"\n",
        "    inps = [tokenizer.encode_plus(t, max_length=max_len, pad_to_max_length=True, add_special_tokens=True) for t in tweets]\n",
        "    inp_tok = np.array([a['input_ids'] for a in inps])\n",
        "    ids = np.array([a['attention_mask'] for a in inps])\n",
        "    segments = np.array([a['token_type_ids'] for a in inps])\n",
        "    return inp_tok, ids, segments\n",
        "\n",
        "def warmup(epoch, lr):\n",
        "    \"\"\"Used for increasing the learning rate slowly, this tends to achieve better convergence.\n",
        "    However, as we are finetuning for few epoch it's not crucial.\n",
        "    \"\"\"\n",
        "    return max(lr +1e-6, 2e-5)\n",
        "\n",
        "def plot_metrics(pred, true_labels):\n",
        "    \"\"\"Plots a ROC curve with the accuracy and the AUC\"\"\"\n",
        "    acc = accuracy_score(true_labels, np.array(pred.flatten() >= .5, dtype='int'))\n",
        "    fpr, tpr, thresholds = roc_curve(true_labels, pred)\n",
        "    auc = roc_auc_score(true_labels, pred)\n",
        "\n",
        "    fig, ax = plt.subplots(1, figsize=(8,8))\n",
        "    ax.plot(fpr, tpr, color='red')\n",
        "    ax.plot([0,1], [0,1], color='black', linestyle='--')\n",
        "    ax.set_title(f\"AUC: {auc}\\nACC: {acc}\");\n",
        "    return fig"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIQ_Ff3nPSYw",
        "outputId": "af3a63cd-efbd-4a43-ac25-fa5d10f125d1"
      },
      "source": [
        "inp_tok, ids, segments = get_inputs(X_train, xlnet_tokenizer)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WhwGNpZPVhV"
      },
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=4, min_delta=0.02, restore_best_weights=True),\n",
        "    tf.keras.callbacks.LearningRateScheduler(warmup, verbose=0),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=1e-6, patience=2, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=1e-6)\n",
        "]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKcm9zTpPW6k",
        "outputId": "3f6a3d99-c69b-4ef2-a87d-736238a5e5f0"
      },
      "source": [
        "hist = xlnet.fit(x=inp_tok, y=y_train, epochs=15, batch_size=16, validation_split=.15, callbacks=callbacks)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "452/452 [==============================] - 739s 2s/step - loss: 0.6134 - accuracy: 0.7006 - precision: 0.6580 - recall: 0.5957 - val_loss: 0.4707 - val_accuracy: 0.7725 - val_precision: 0.7594 - val_recall: 0.6747\n",
            "Epoch 2/15\n",
            "452/452 [==============================] - 742s 2s/step - loss: 0.5299 - accuracy: 0.7524 - precision: 0.7108 - recall: 0.6904 - val_loss: 0.5747 - val_accuracy: 0.7812 - val_precision: 0.8509 - val_recall: 0.5836\n",
            "Epoch 3/15\n",
            "452/452 [==============================] - 741s 2s/step - loss: 0.4410 - accuracy: 0.7960 - precision: 0.7544 - recall: 0.7614 - val_loss: 0.4466 - val_accuracy: 0.7961 - val_precision: 0.7633 - val_recall: 0.7491\n",
            "Epoch 4/15\n",
            "452/452 [==============================] - 742s 2s/step - loss: 0.4050 - accuracy: 0.8230 - precision: 0.7875 - recall: 0.7914 - val_loss: 0.6168 - val_accuracy: 0.8008 - val_precision: 0.7088 - val_recall: 0.8959\n",
            "Epoch 5/15\n",
            "452/452 [==============================] - 742s 2s/step - loss: 0.3340 - accuracy: 0.8599 - precision: 0.8272 - recall: 0.8419 - val_loss: 0.6802 - val_accuracy: 0.7325 - val_precision: 0.8556 - val_recall: 0.4405\n",
            "Epoch 6/15\n",
            "452/452 [==============================] - 743s 2s/step - loss: 0.2717 - accuracy: 0.8893 - precision: 0.8627 - recall: 0.8752 - val_loss: 0.7128 - val_accuracy: 0.7647 - val_precision: 0.8521 - val_recall: 0.5353\n",
            "Epoch 7/15\n",
            "452/452 [==============================] - 742s 2s/step - loss: 0.2117 - accuracy: 0.9178 - precision: 0.8960 - recall: 0.9096 - val_loss: 0.7726 - val_accuracy: 0.7945 - val_precision: 0.7924 - val_recall: 0.6952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "bnV9T5SoPY6R",
        "outputId": "1cc05fbe-a990-4e54-f1fd-202ec2417825"
      },
      "source": [
        "inp_tok, ids, segments = get_inputs(X_test, xlnet_tokenizer)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-46e8e4f08300>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minp_tok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxlnet_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'get_inputs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-jms4QHPakA"
      },
      "source": [
        "preds = xlnet.predict(inp_tok, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFbX4JlaPcpE"
      },
      "source": [
        "plot_metrics(preds, y_test);"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_2_xlnet.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gibsonx/CE888/blob/master/Assignment/Assignment_2_xlnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk0hIUcdXpi2"
      },
      "source": [
        "!pip install -U tensorflow tensorflow_datasets tensorflow_text zhon sentencepiece transformers focal-loss tfa-nightly\n",
        "!git clone https://github.com/cardiffnlp/tweeteval.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WFgb6kZXX8C"
      },
      "source": [
        "import os,pathlib,re,nltk,pickle,string\n",
        "nltk.download('punkt')\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as text\n",
        "from tensorflow.keras import Input, layers, losses, preprocessing, utils\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.layers.core import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix\n",
        "from zhon import hanzi \n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n",
        "import csv\n",
        "from tqdm.notebook import tqdm\n",
        "from nltk.tokenize import word_tokenize\n",
        "from focal_loss import BinaryFocalLoss\n",
        "# import tokenization\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# import tensorflow_hub as hub\n",
        "# import tensorflow_datasets as tfds\n",
        "# from collections import Counter\n",
        "# from nltk.stem import WordNetLemmatizer\n",
        "# from google.colab import output\n",
        "# from google.colab import drive\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix\n",
        "from zhon import hanzi \n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import csv\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from sklearn.metrics import f1_score\n",
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEXTeQYlY9Ss"
      },
      "source": [
        "# drive.mount('/content/drive/')\n",
        "base_dir = '/content/tweeteval/datasets/'\n",
        "hate_dir = base_dir + \"hate\"\n",
        "irony_dir = base_dir + \"irony\"\n",
        "offensive_dir = base_dir + \"offensive\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXBfnagPXojl"
      },
      "source": [
        "def imbalance_under_sampling(dfname):\n",
        "  df_label_a = dfname[dfname['label'] == 1]\n",
        "  df_label_b = dfname[dfname['label'] == 0]\n",
        "  if df_label_a.shape[0] > df_label_b.shape[0]:\n",
        "  # count_class_0, count_class_1 = dfname.label.value_counts()\n",
        "    df_label_a = df_label_a.sample(df_label_b.shape[0],random_state=1)\n",
        "    df = pd.concat([df_label_b, df_label_a], axis=0)\n",
        "    print('label 0 is more',df.label.value_counts())  \n",
        "    return df\n",
        "  else: \n",
        "    df_label_b = df_label_b.sample(df_label_a.shape[0],random_state=1)\n",
        "    df = pd.concat([df_label_b, df_label_a], axis=0)\n",
        "    print('label 1 is more',df.label.value_counts())  \n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcveRCcrB1Kh"
      },
      "source": [
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    \n",
        "    return emoji_pattern.sub(r'', string)\n",
        "\n",
        "def preprocess_tweet_text(tweet):\n",
        "    tweet = tweet.lower()\n",
        "    # Remove urls\n",
        "    tweet = re.sub(r\"^#\\S+|\\s#\\S+\", '', tweet, flags=re.MULTILINE)\n",
        "    # Remove user @ references and '#' from tweet\n",
        "    tweet = re.sub(r\"^@\\S+|\\s@\\S+\",'', tweet)\n",
        "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n",
        "    # Remove punctuations\n",
        "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
        "    tweet = tweet.translate(str.maketrans('', '', hanzi.punctuation))\n",
        "    tweet = remove_emoji(tweet)\n",
        "    # tweet_tokens = word_tokenize(tweet)\n",
        "    # # Remove stopwords\n",
        "    # filtered_words = [w for w in tweet_tokens]\n",
        "    # return \" \".join(filtered_words)\n",
        "    return tweet\n",
        "\n",
        "def readfile(text):\n",
        "  pd_list = []\n",
        "  with open(text,'r') as f:\n",
        "    for tweet in f.read().splitlines():\n",
        "      tweet = preprocess_tweet_text(tweet)\n",
        "      pd_list.append(tweet)   \n",
        "  return pd_list\n",
        " \n",
        "def readfile_label(text):\n",
        "  pd_list = []\n",
        "  with open(text,'r') as f:\n",
        "    for tweet in f.read().splitlines():\n",
        "      pd_list.append(int(tweet))  \n",
        "  return pd_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q0IWHEhEScP"
      },
      "source": [
        "#Pre-process csv and merge into a Dataframe\n",
        "#Hate DataFrame Data \n",
        "hate_dict_train = {'text': readfile(os.path.join(hate_dir,\"train_text.txt\")),'label': readfile_label(os.path.join(hate_dir,\"train_labels.txt\"))}\n",
        "hate_dict_val = {'text': readfile(os.path.join(hate_dir,\"val_text.txt\")),'label': readfile_label(os.path.join(hate_dir,\"val_labels.txt\"))}\n",
        "hate_dict_test = {'text': readfile(os.path.join(hate_dir,\"test_text.txt\")),'label': readfile_label(os.path.join(hate_dir,\"test_labels.txt\"))}\n",
        "hate_train_df = imbalance_under_sampling(pd.DataFrame(hate_dict_train))\n",
        "hate_val_df = pd.DataFrame(hate_dict_val)\n",
        "hate_test_df = pd.DataFrame(hate_dict_test)\n",
        "hate_frames = [hate_train_df, hate_val_df, hate_test_df]\n",
        "df_hate = pd.concat(hate_frames)\n",
        "not_hate = df_hate[df_hate['label'] == 0]\n",
        "hate = df_hate[df_hate['label'] == 1]\n",
        "\n",
        "#Irony DataFrame\n",
        "irony_dict_train = {'text': readfile(os.path.join(irony_dir,\"train_text.txt\")),'label': readfile_label(os.path.join(irony_dir,\"train_labels.txt\"))}\n",
        "irony_dict_val = {'text': readfile(os.path.join(irony_dir,\"val_text.txt\")),'label': readfile_label(os.path.join(irony_dir,\"val_labels.txt\"))}\n",
        "irony_dict_test = {'text': readfile(os.path.join(irony_dir,\"test_text.txt\")),'label': readfile_label(os.path.join(irony_dir,\"test_labels.txt\"))}\n",
        "irony_train_df = imbalance_under_sampling(pd.DataFrame(irony_dict_train))\n",
        "irony_val_df = pd.DataFrame(irony_dict_val)\n",
        "irony_test_df = pd.DataFrame(irony_dict_test)\n",
        "irony_frames = [irony_train_df,irony_val_df,irony_test_df]\n",
        "df_irony = pd.concat(irony_frames)\n",
        "not_irony  = df_irony[df_irony['label'] == 0]\n",
        "irony  = df_irony[df_irony['label'] == 1]\n",
        "\n",
        "#Offensive DataFrame\n",
        "offensive_dict_train = {'text': readfile(os.path.join(offensive_dir,\"train_text.txt\")),'label': readfile_label(os.path.join(offensive_dir,\"train_labels.txt\"))}\n",
        "offensive_dict_val = {'text': readfile(os.path.join(offensive_dir,\"val_text.txt\")),'label': readfile_label(os.path.join(offensive_dir,\"val_labels.txt\"))}\n",
        "offensive_dict_test = {'text': readfile(os.path.join(offensive_dir,\"test_text.txt\")),'label': readfile_label(os.path.join(offensive_dir,\"test_labels.txt\"))}\n",
        "offensive_train_df = imbalance_under_sampling(pd.DataFrame(offensive_dict_train))\n",
        "offensive_val_df = pd.DataFrame(offensive_dict_val)\n",
        "offensive_test_df = pd.DataFrame(offensive_dict_test)\n",
        "offensive_frames = [offensive_train_df,offensive_val_df,offensive_test_df]\n",
        "df_offensive = pd.concat(offensive_frames)\n",
        "not_offensive  = df_offensive[df_offensive['label'] == 0]\n",
        "offensive  = df_offensive[df_offensive['label'] == 1]\n",
        "\n",
        "hate = df_hate[df_hate['label'] == 1]\n",
        "hate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_18embbHU9R"
      },
      "source": [
        "from transformers import TFXLNetModel, XLNetTokenizer\n",
        "xlnet_tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')\n",
        "xlnet_model = TFXLNetModel.from_pretrained('xlnet-large-cased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqnGi96JHa6S"
      },
      "source": [
        "def create_xlnet(xl_model):\n",
        "    \"\"\" Creates the model. It is composed of the XLNet main block and then\n",
        "    a classification head its added\n",
        "    \"\"\"\n",
        "    # Define token ids as inputs\n",
        "    word_ids = tf.keras.Input(shape=(120,), name='word_ids', dtype='int32')\n",
        "    word_attention = tf.keras.Input(shape=(120,), name='word_attention', dtype='int32')\n",
        "    # word_seq = tf.keras.Input(shape=(120,), name='word_seq', dtype='int32')\n",
        "\n",
        "    # Call XLNet model\n",
        "    xlnet_encodings = xl_model([word_ids,word_attention])[0]\n",
        "\n",
        "    # CLASSIFICATION HEAD \n",
        "    # Collect last step from last hidden state (CLS)\n",
        "    doc_encoding = tf.squeeze(xlnet_encodings[:, -1:, :], axis=1)\n",
        "    # Apply dropout for regularization\n",
        "    dense = tf.keras.layers.Dense(32, activation='relu', name='encoding')(doc_encoding)\n",
        "    drop = tf.keras.layers.Dropout(0.1)(dense)\n",
        "    # Final output \n",
        "    outputs = tf.keras.layers.Dense(1, activation='sigmoid', name='outputs')(drop)\n",
        "\n",
        "    # Compile model\n",
        "    model = tf.keras.Model(inputs=[word_ids,word_attention], outputs=[outputs])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=2e-5), loss=BinaryFocalLoss(gamma=2), metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPK_vhDmPP8a"
      },
      "source": [
        "def get_inputs(tweets, tokenizer, max_len=120):\n",
        "    \"\"\" Gets tensors from text using the tokenizer provided\"\"\"\n",
        "    inps = [tokenizer.encode_plus(t, max_length=max_len, pad_to_max_length=True, add_special_tokens=True) for t in tweets]\n",
        "    inp_tok = np.array([a['input_ids'] for a in inps])\n",
        "    ids = np.array([a['attention_mask'] for a in inps])\n",
        "    return inp_tok, ids\n",
        "\n",
        "def plot_metrics(pred, true_labels):\n",
        "    \"\"\"Plots a ROC curve with the accuracy and the AUC\"\"\"\n",
        "    acc = accuracy_score(true_labels, np.array(pred.flatten() >= .5, dtype='int'))\n",
        "    fpr, tpr, thresholds = roc_curve(true_labels, pred)\n",
        "    auc = roc_auc_score(true_labels, pred)\n",
        "\n",
        "    fig, ax = plt.subplots(1, figsize=(8,8))\n",
        "    ax.plot(fpr, tpr, color='red')\n",
        "    ax.plot([0,1], [0,1], color='black', linestyle='--')\n",
        "    ax.set_title(f\"AUC: {auc}\\nACC: {acc}\");\n",
        "    return fig\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, min_delta=0.001, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=1e-6, patience=2, verbose=0, mode='auto', min_delta=0.001, cooldown=0, min_lr=1e-6)\n",
        "]    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXwprO6XGdkt"
      },
      "source": [
        "def plot_confusion_matrix(cm,target_names,title='Confusion matrix',cmap=None,normalize=True):\n",
        "  accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "  misclass = 1 - accuracy\n",
        "\n",
        "  if cmap is None:\n",
        "      cmap = plt.get_cmap('Blues')\n",
        "\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "  plt.title(title)\n",
        "  plt.colorbar()\n",
        "\n",
        "  if target_names is not None:\n",
        "      tick_marks = np.arange(len(target_names))\n",
        "      plt.xticks(tick_marks, target_names, rotation=45)\n",
        "      plt.yticks(tick_marks, target_names)\n",
        "\n",
        "  if normalize:\n",
        "      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "\n",
        "  thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
        "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "      if normalize:\n",
        "          plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
        "                    horizontalalignment=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "      else:\n",
        "          plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                    horizontalalignment=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "  plt.tight_layout()\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bj3OjhaZp2Mn"
      },
      "source": [
        "hate_train_input = get_inputs(hate_train_df['text'], xlnet_tokenizer)\n",
        "hate_val_input = get_inputs(hate_val_df['text'], xlnet_tokenizer)\n",
        "hate_test_input = get_inputs(hate_test_df['text'], xlnet_tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pj9cArjqp2Pw"
      },
      "source": [
        "xlnet_hate = create_xlnet(xlnet_model)\n",
        "xlnet_hate.summary()\n",
        "hist = xlnet_hate.fit(x=hate_train_input, y=hate_train_df['label'], epochs=20, batch_size=16, validation_data=(hate_val_input, hate_val_df['label']), callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZltEvvbVp2XG"
      },
      "source": [
        "preds = xlnet_hate.predict(hate_test_input, verbose=True)\n",
        "y_pred = [i[0] for i in preds.round().astype(int)]\n",
        "cm = confusion_matrix(hate_test_df['label'],y_pred)\n",
        "plot_confusion_matrix(cm, normalize=False,target_names=['Not_Hate', 'hate'],title=\"Confusion Matrix for hate\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DnYphiSukFS"
      },
      "source": [
        "f1_score(hate_test_df.label,y_pred,average='macro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ankQ0HjGyvW2"
      },
      "source": [
        "irony_train_input = get_inputs(irony_train_df['text'], xlnet_tokenizer)\n",
        "irony_val_input = get_inputs(irony_val_df['text'], xlnet_tokenizer)\n",
        "irony_test_input = get_inputs(irony_test_df['text'], xlnet_tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDDqsFVDyvqN"
      },
      "source": [
        "xlnet_irony = create_xlnet(xlnet_model)\n",
        "hist = xlnet_irony.fit(x=irony_train_input, y=irony_train_df['label'], epochs=2, batch_size=16, validation_data=(irony_val_input, irony_val_df['label']), callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyMdT0Xwyv2J"
      },
      "source": [
        "preds = xlnet_irony.predict(irony_test_input, verbose=True)\n",
        "y_pred = [i[0] for i in preds.round().astype(int)]\n",
        "cm = confusion_matrix(irony_test_df['label'],y_pred)\n",
        "plot_confusion_matrix(cm, normalize=False,target_names=['Not_irony ', 'irony'],title=\"Confusion Matrix for irony\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLIxSI7-ywDP"
      },
      "source": [
        "f1_score(irony_test_df.label,y_pred,average='macro')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIQ_Ff3nPSYw"
      },
      "source": [
        "offensive_train_input = get_inputs(offensive_train_df['text'], xlnet_tokenizer)\n",
        "offensive_val_input = get_inputs(offensive_val_df['text'], xlnet_tokenizer)\n",
        "offensive_test_input = get_inputs(offensive_test_df['text'], xlnet_tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKY0NwyhuqAW"
      },
      "source": [
        "xlnet = create_xlnet(xlnet_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKcm9zTpPW6k"
      },
      "source": [
        "hist = xlnet.fit(x=offensive_train_input, y=offensive_train_df['label'], epochs=3, batch_size=16, validation_data=(offensive_val_input, offensive_val_df['label']), callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-jms4QHPakA"
      },
      "source": [
        "preds = xlnet.predict(offensive_test_input, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFbX4JlaPcpE"
      },
      "source": [
        "# plot_metrics(preds, y_test)\n",
        "y_pred = [i[0] for i in preds.round().astype(int)]\n",
        "cm = confusion_matrix(offensive_test_df['label'],y_pred)\n",
        "plot_confusion_matrix(cm, normalize=False,target_names=['Not_offensive', 'offensive'],title=\"Confusion Matrix for offensive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d_aIxVq4btJ"
      },
      "source": [
        "xlnet.evaluate(offensive_test_input,offensive_test_df['label'], verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEMloMrpRLU3"
      },
      "source": [
        "f1_score(offensive_test_df.label,y_pred,average='macro')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}